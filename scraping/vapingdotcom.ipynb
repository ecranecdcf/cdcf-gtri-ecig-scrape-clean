{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dd06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()\n",
    "\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from ecig_parsing import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06030f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url, clicked=False, closed=False, elements=True):\n",
    "    # print('GET', url)\n",
    "    driver = None\n",
    "    try:\n",
    "#         # Set up headless mode for Firefox\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')  # Enable headless mode properly\n",
    "\n",
    "        # Start the WebDriver in headless mode\n",
    "        driver = webdriver.Firefox(options=options)\n",
    "#         driver = webdriver.Firefox()\n",
    "\n",
    "\n",
    "        # Start the WebDriver in headless mode\n",
    "        driver.get(url)\n",
    "        last_n = 0\n",
    "        hrefs = set()\n",
    "        button_id = \"yes-button\"\n",
    "\n",
    "        html = driver.page_source\n",
    "        time.sleep(1)\n",
    "        same_count = 0\n",
    "        if not clicked:\n",
    "            wait = WebDriverWait(driver, 15)  # 10 seconds timeout\n",
    "            button = wait.until(EC.element_to_be_clickable((By.ID, button_id)))  # Use ID, XPATH, or other locator\n",
    "\n",
    "            # Click the button\n",
    "            button.click()\n",
    "            clicked = True\n",
    "#         if not closed:\n",
    "#             wait = WebDriverWait(driver, 10)  # 10 seconds timeout\n",
    "#             button = wait.until(EC.element_to_be_clickable((By.ID, 'omnisend-form-65006a9819ed2c118d9a9352-close-action')))  # Use ID, XPATH, or other locator\n",
    "\n",
    "#             # Click the button\n",
    "#             button.click()\n",
    "#             closed = True\n",
    "    \n",
    "\n",
    "        # Set a delay for dynamic content loading\n",
    "        SCROLL_PAUSE_TIME = 0.5  # Shorter pause for smoother scroll\n",
    "\n",
    "        # Scroll incrementally by a small step (e.g., 500 pixels)\n",
    "        scroll_increment = 500\n",
    "\n",
    "        # Get the initial scroll height\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        first_height = last_height\n",
    "        # print(last_height)\n",
    "\n",
    "        while True:\n",
    "            # Scroll down by the increment\n",
    "            driver.execute_script(f\"window.scrollBy(0, {scroll_increment});\")\n",
    "\n",
    "            # Wait for the page to load new content\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "            # Calculate the new scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            # Check if new elements have appeared on the screen\n",
    "            # You can check specific elements like this:\n",
    "            # elements = driver.find_elements(By.CLASS_NAME, \"your-element-class\")\n",
    "\n",
    "            # If the scroll height has not changed, break the loop\n",
    "            #print(same_count, new_height, last_height)\n",
    "            if new_height == last_height :\n",
    "                same_count += 1\n",
    "                if same_count >= 3:\n",
    "                    break\n",
    "            else:\n",
    "                same_count = 0\n",
    "\n",
    "            # Update the last height\n",
    "            last_height = new_height\n",
    "            \n",
    "            html = driver.page_source\n",
    "            #print(html)\n",
    "        \n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        html = ''\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97f271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Specify the file path\n",
    "json_file_path = 'vapingdotcom.json'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(json_file_path):\n",
    "    # Open and read the JSON file\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        try:\n",
    "            found = json.load(file)\n",
    "            print(\"JSON data:\", len(found.keys()), 'items')\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error: The file contains invalid JSON.\")\n",
    "else:\n",
    "    found = dict()\n",
    "    print(f\"File '{json_file_path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b3ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "links = [\n",
    "   'https://vaping.com/collections/all-products' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5086bc7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "product_list = list()\n",
    "all_headers = list()\n",
    "header_counter = Counter(all_headers)\n",
    "product_tags = set()\n",
    "header_samples = dict()\n",
    "\n",
    "\n",
    "def save():\n",
    "    print('SAVING...')\n",
    "    with open('vapingdotcom_scrape.csv', mode='w') as file:\n",
    "        # Create a DictWriter object\n",
    "        writer = csv.DictWriter(file, fieldnames=product_list[0].keys())\n",
    "\n",
    "        # Write the header (column names)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Write the rows (each dictionary in the data list)\n",
    "        writer.writerows(product_list)\n",
    "        \n",
    "    with open(json_file_path, 'w') as file:\n",
    "        json.dump(found, file, indent=4) \n",
    "        \n",
    "max_pages = 75\n",
    "found_max = False\n",
    "for link_index,l in enumerate(links):\n",
    "    page = 1\n",
    "    while True:\n",
    "        if page > max_pages:\n",
    "            break\n",
    "        main_url = f'{l}?page={page}'\n",
    "        try:\n",
    "            if main_url in found:\n",
    "                req_text = found[main_url]\n",
    "            else:\n",
    "                req_text = get_html(main_url)\n",
    "                found[main_url] = req_text\n",
    "\n",
    "            soup = BeautifulSoup(req_text)\n",
    "            prods = soup.find_all('product-card')\n",
    "            \n",
    "\n",
    "        except Exception as ex:\n",
    "            print(ex, 'FAILED', main_url)\n",
    "            break\n",
    "           \n",
    "            \n",
    "        total_items = len(prods)\n",
    "        print('LINK INDEX', link_index, 'OF', len(links), ' -- PAGE', page, 'OF', max_pages, f'({total_items})', main_url)\n",
    "\n",
    "            \n",
    "        page += 1\n",
    "        if total_items == 0:\n",
    "            break\n",
    "            \n",
    "        for psoup in prods:\n",
    "            txt = psoup.get_text().strip()\n",
    "            if txt == '':\n",
    "                continue\n",
    "                \n",
    "            product_name = psoup.find('a', class_='bold').text.strip()\n",
    "\n",
    "            # Extract the product link\n",
    "            product_link = psoup.find('a', class_='bold')['href']\n",
    "\n",
    "            # Extract the sale price\n",
    "            sale_price = psoup.find('sale-price').text.replace('Sale price', '').strip()\n",
    "\n",
    "            # Extract the primary image URL and alt text\n",
    "            primary_image_tag = psoup.find('img', class_='product-card__image--primary')\n",
    "            if primary_image_tag:\n",
    "                primary_image_url = primary_image_tag['src']\n",
    "                primary_image_alt = primary_image_tag['alt']\n",
    "            else:\n",
    "                primary_image_url = None\n",
    "                primary_image_alt = None\n",
    "\n",
    "            # Extract the secondary image URL and alt text\n",
    "            secondary_image_tag = psoup.find('img', class_='product-card__image--secondary')\n",
    "            if secondary_image_tag:\n",
    "                secondary_image_url = secondary_image_tag['src']\n",
    "                secondary_image_alt = secondary_image_tag['alt']\n",
    "            else:\n",
    "                secondary_image_url = None\n",
    "                secondary_image_alt = None\n",
    "            \n",
    "            if product_link.startswith('http'):\n",
    "                full_link = product_link\n",
    "            else:\n",
    "                full_link = f'https://vaping.com{product_link}'\n",
    "                \n",
    "            if primary_image_url and primary_image_url.startswith('//'):\n",
    "                primary_image_url = 'http:' + primary_image_url\n",
    "                \n",
    "            if secondary_image_url and secondary_image_url.startswith('//'):\n",
    "                secondary_image_url = 'http:' + secondary_image_url\n",
    "            \n",
    "            tag = full_link.split('/')[-1]\n",
    "            if tag == '':\n",
    "                tag = full_link.split('/')[-2]\n",
    "                \n",
    "            print(tag, full_link)\n",
    "\n",
    "            if tag in product_tags:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "            \n",
    "                if full_link in found:\n",
    "                    reqtxt = found[full_link]\n",
    "                else:\n",
    "                    # print(full_link)\n",
    "                    reqtxt = get_html(full_link, elements=False)\n",
    "                    found[full_link] = reqtxt\n",
    "            except Exception as ex:\n",
    "                print(ex, 'FAILED', main_url)\n",
    "                continue\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "            soup = BeautifulSoup(reqtxt)\n",
    "            \n",
    "\n",
    "            desc_section = soup.find('div', {'class': 'product-info__description'})\n",
    "            section_map = parse_description_sections(desc_section, all_headers, header_samples, full_link)\n",
    "                            \n",
    "            fieldsets = soup.find_all('fieldset', {'class': 'variant-picker__option'})\n",
    "            size_values = []\n",
    "            nicotine_values = []\n",
    "            color_values = []\n",
    "            for fs in fieldsets:\n",
    "                txt = fs.get_text().strip()\n",
    "                if 'Nicotine:' in txt:\n",
    "                    radio_inputs = fs.find_all('input', {'type': 'radio'})\n",
    "\n",
    "                    # Extract values from the input elements\n",
    "                    nicotine_values = [input_tag['value'] for input_tag in radio_inputs]\n",
    "                elif 'Size:' in txt:\n",
    "                    radio_inputs = fs.find_all('input', {'type': 'radio'})\n",
    "\n",
    "                    # Extract values from the input elements\n",
    "                    size_values = [input_tag['value'] for input_tag in radio_inputs]\n",
    "                elif 'Color:' in txt:\n",
    "                    radio_inputs = fs.find_all('input', {'type': 'radio'})\n",
    "\n",
    "                    # Extract values from the input elements\n",
    "                    color_values = [input_tag['value'] for input_tag in radio_inputs]\n",
    "                \n",
    "                \n",
    "            brand_div = soup.find('div', {'class': 'product-info__vendor'})\n",
    "            if brand_div:\n",
    "                brand_name = brand_div.get_text().strip()\n",
    "            else:\n",
    "                brand_name = ''\n",
    "                \n",
    "            stock_div = soup.find('div', {'class': 'product-info__inventory'})\n",
    "            if stock_div:\n",
    "                stock_status = stock_div.get_text().strip()\n",
    "            else:\n",
    "                stock_status = ''\n",
    "                \n",
    "            warning_div = soup.find('div', {'class': 'prop-65-text'})\n",
    "            if warning_div:\n",
    "                warning = warning_div.get_text().strip()\n",
    "            else:\n",
    "                warning = ''\n",
    "                \n",
    "            full_image_urls = []\n",
    "            thumbnails = soup.find('scroll-shadow', {'class': 'product-gallery__thumbnail-list-wrapper'})\n",
    "            if thumbnails:\n",
    "                # Find all img tags and extract src attribute\n",
    "                image_urls = [img['src'] for img in thumbnails.find_all('img')]\n",
    "\n",
    "                # Prefix the URLs with \"https:\" to make them fully accessible\n",
    "                for u in image_urls:\n",
    "                    if 'svg' in u:\n",
    "                        continue\n",
    "                    if u.startswith('//'):\n",
    "                        u = \"https:\" + u\n",
    "                    full_image_urls.append(u)\n",
    "            \n",
    "            product_data = {\n",
    "                'tag': tag,\n",
    "                \"title\": product_name,\n",
    "                'brand': brand_name,\n",
    "                'stock_status': stock_status,\n",
    "                \"link\": full_link,\n",
    "                \"price\": sale_price,\n",
    "                'image_url': primary_image_url,\n",
    "                'image_alt_text': primary_image_alt,\n",
    "                'image2_url': secondary_image_url,\n",
    "                'image2_alt_text': secondary_image_alt,\n",
    "                'image_urls': full_image_urls,\n",
    "                'sizes': size_values,\n",
    "                'nicotine_values': nicotine_values,\n",
    "                'colors': color_values,\n",
    "                'warning': warning,\n",
    "\n",
    "            }\n",
    "            \n",
    "            if primary_image_url:\n",
    "                download_image(primary_image_url, f'{tag}', save_dir='vapingdotcom_images')\n",
    "                \n",
    "            if secondary_image_url:\n",
    "                download_image(secondary_image_url, f'{tag}_secondary', save_dir='vapingdotcom_images')\n",
    "\n",
    "            if full_image_urls:\n",
    "                image_num = 0\n",
    "                for i in product_data['image_urls']:\n",
    "                    if i and i != '':\n",
    "                        download_image(i, f'{tag}-image{image_num}', save_dir='vapingdotcom_images')\n",
    "                        image_num += 1\n",
    "                    \n",
    "            desc_fields = ''\n",
    "            for s, v in section_map.items():\n",
    "                if 'description' not in s:\n",
    "                    s = s + '_description'\n",
    "                val = v.replace('\\xa0', ' ').strip()\n",
    "                \n",
    "                if val.startswith(':'):\n",
    "                    val = val[1:].strip()\n",
    "            \n",
    "                product_data[s] = val\n",
    "                if product_data[s] != '' and 'link' not in s:\n",
    "                    desc_fields += f'\\n{product_data[s]}'\n",
    "\n",
    "            feats = list()\n",
    "            feat = find_features(desc_fields)\n",
    "            any_found, puffs_res, nico_res, ml_res, flav_text, dev_text = feat\n",
    "\n",
    "            if any_found:\n",
    "                #print(feat)\n",
    "                feats.append(feat)\n",
    "            disposable,recharge,battery,mesh,usb,adjustable,found_flavs = features_to_cats(feats)\n",
    "            product_data['disposable'] = disposable\n",
    "            product_data['rechargeable'] = recharge\n",
    "            product_data['battery'] = battery\n",
    "            product_data['mesh'] = mesh\n",
    "            product_data['usb'] = usb\n",
    "            product_data['adjustable'] = adjustable\n",
    "\n",
    "            product_list.append(product_data)\n",
    "            product_tags.add(tag)\n",
    "        \n",
    "            if len(product_list) % 50 == 0:\n",
    "                rc = (random.choice(product_list))\n",
    "                print('{')\n",
    "                for key, value in rc.items():\n",
    "                    if not value:\n",
    "                        continue\n",
    "\n",
    "                    if isinstance(value, list):\n",
    "                        print(f\"\\t{key}: {len(value)} items\")\n",
    "                    else:\n",
    "                        value = value.replace('\\n', ' ')\n",
    "\n",
    "                        if key == 'image_url' or key == 'link' or key == 'title'  or key == 'tag' :\n",
    "                            print(f\"\\t{key}: {value}\")\n",
    "                        elif 1 <= len(value) <= 20:\n",
    "                            print(f\"\\t{key}: {value}\")\n",
    "                        elif len(value) > 20:\n",
    "                            print(f\"\\t{key}: {value[:20]}...\")\n",
    "                print('}')\n",
    "        \n",
    "                print(len(product_list), 'ITEMS FOUND')\n",
    "                print()\n",
    "                print()\n",
    "                for item, count in header_counter.most_common(50):\n",
    "                    if count > 5:\n",
    "                        print(f\"{item}: {count} - {header_samples[item]}\")\n",
    "                print()\n",
    "                print()\n",
    "\n",
    "\n",
    "            \n",
    "        save()\n",
    "    save()\n",
    "save()\n",
    "        \n",
    "random.choice(product_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd291544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647f4521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca1128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
